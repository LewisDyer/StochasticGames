
@inproceedings{milazzo_case_2015,
	title = {Case {Studies} of {Application} of {Probabilistic} and {Statistical} {Model} {Checking} in {Game} {Design}},
	doi = {10.1109/GAS.2015.13},
	abstract = {We propose probabilistic/statistical model checking as a tool for game design. Models of games under design can be used for the early evaluation of properties related with game duration, existence of different successful strategies, and balancing of the game core mechanics. To show the usefulness of model checking in game design we consider and develop three case studies in which model checking is used to answer specific game design questions.},
	booktitle = {2015 {IEEE}/{ACM} 4th {International} {Workshop} on {Games} and {Software} {Engineering}},
	author = {Milazzo, Paolo and Pardini, Giovanni and Sestini, Dario and Bove, Pasquale},
	month = may,
	year = {2015},
	keywords = {Boards, Collaboration, computer games, formal verification, game core mechanics, game design questions, Games, Model checking, Probabilistic logic, probabilistic model checking, Probability, statistical analysis, statistical model checking, Tracking},
	pages = {29--35},
	file = {Submitted Version:C\:\\Users\\Lewis\\Zotero\\storage\\X63N22L6\\Milazzo et al. - 2015 - Case Studies of Application of Probabilistic and S.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Lewis\\Zotero\\storage\\8EQRH573\\7169466.html:text/html}
}

@inproceedings{kavanagh_balancing_2019,
	title = {Balancing {Turn}-{Based} {Games} with {Chained} {Strategy} {Generation}},
	doi = {10.1109/tg.2019.2943227},
	abstract = {Probabilistic model checking can overcome much of the complexity inherent in balancing games. Game balancing is the careful maintenance of relationships between the ways in which a game can be played, to ensure no single way is strictly better than all others and that players are offered a wide variety of ways to play successfully. We introduce a novel approach towards automating game balancing using probabilistic model checking called chained strategy generation (CSG). This involves generating chains of adversarial strategies which mimic the way players adapt their approach during repeated plays of a game. We use CSG to map out the evolving metagame. The trends identified can allow game developers to identify strategies which will be too strong and ways of playing the game which a player may want to use, but are never viable for successful competitive play. We introduce a case study, a game called RPGLite, and use CSG to compare five candidate configurations for the game. We show how to determine which configurations of RPGLite lead to a more fair and interesting experience for players. We also identify unexpected trends in how the strategies evolve. Our approach introduces a new technique for improving game development and player experience.},
	author = {Kavanagh, W. and Miller, A. and Norman, G. and Andrei, Oana},
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\VEBIMRZZ\\Kavanagh et al. - 2019 - Balancing Turn-Based Games with Chained Strategy G.pdf:application/pdf}
}

@inproceedings{jaffe_evaluating_2012,
	address = {Stanford, California, USA},
	series = {{AIIDE}'12},
	title = {Evaluating competitive game balance with restricted play},
	abstract = {Game balancing is the fine-tuning phase in which a functioning game is adjusted to be deep, fair, and interesting. Balancing is difficult and time-consuming, as designers must repeatedly tweak parameters, and run lengthy playtests to evaluate the effects of these changes. If designers could receive immediate feedback on their designs, they could explore a vast space of variations, and select only the most promising games for playtesting. Such automated design feedback has been difficult to achieve, as there is no mathematical formulation of game balance that unifies many of its forms. We argue for a formulation in which carefully restricted agents are played against standard agents. We develop this restricted-play balance framework, and evaluate its utility by building a tool capable of calculating measures of balance for a large family of games. By applying this tool to an educational card game, we demonstrate how the framework and tool allow designers to rapidly evaluate and iterate on the balance of their games.},
	urldate = {2020-10-20},
	booktitle = {Proceedings of the {Eighth} {AAAI} {Conference} on {Artificial} {Intelligence} and {Interactive} {Digital} {Entertainment}},
	publisher = {AAAI Press},
	author = {Jaffe, Alexander and Miller, Alex and Andersen, Erik and Liu, Yun-En and Karlin, Anna and Popović, Zoran},
	month = oct,
	year = {2012},
	pages = {26--31},
	file = {Full Text:C\:\\Users\\Lewis\\Zotero\\storage\\LGPZ44TS\\Jaffe et al. - 2012 - Evaluating competitive game balance with restricte.pdf:application/pdf}
}

@inproceedings{hom_automatic_2007,
	address = {Stanford, California},
	series = {{AIIDE}'07},
	title = {Automatic design of balanced board games},
	abstract = {AI techniques are already widely used in game software to provide computer-controlled opponents for human players. However, game design is a more-challenging problem than game play. Designers typically expend great effort to ensure that their games are balanced and challenging. Dynamic game-balancing techniques have been developed to modify a game-engine's parameters in response to user play. In this paper we describe a first attempt at using AI techniques to design balanced board games like checkers and Go by modifying the rules of the game, not just the rule parameters. Our approach involves the use of a commercial general game-playing (GGP) engine that plays according to rules that are specified in a general game-definition language. We use a genetic algorithm (GA) to search the space of game rules, looking for turn-based board games that are well balanced, i.e., those that the GGP engine in self-play finds equally hard to win from either side and rarely draws. The GA finds better games than a random-search strategy that uses equivalent computational effort.},
	urldate = {2020-11-19},
	booktitle = {Proceedings of the {Third} {AAAI} {Conference} on {Artificial} {Intelligence} and {Interactive} {Digital} {Entertainment}},
	publisher = {AAAI Press},
	author = {Hom, Vincent and Marks, Joe},
	month = jun,
	year = {2007},
	pages = {25--30}
}

@article{hansson_logic_1994,
	title = {A logic for reasoning about time and reliability},
	volume = {6},
	issn = {1433-299X},
	url = {https://doi.org/10.1007/BF01211866},
	doi = {10.1007/BF01211866},
	abstract = {We present a logic for stating properties such as, “after a request for service there is at least a 98\% probability that the service will be carried out within 2 seconds”. The logic extends the temporal logic CTL by Emerson, Clarke and Sistla with time and probabilities. Formulas are interpreted over discrete time Markov chains. We give algorithms for checking that a given Markov chain satisfies a formula in the logic. The algorithms require a polynomial number of arithmetic operations, in size of both the formula and the Markov chain. A simple example is included to illustrate the algorithms.},
	language = {en},
	number = {5},
	urldate = {2021-01-29},
	journal = {Formal Aspects of Computing},
	author = {Hansson, Hans and Jonsson, Bengt},
	month = sep,
	year = {1994},
	pages = {512--535},
	file = {Springer Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\5EER7QCY\\Hansson and Jonsson - 1994 - A logic for reasoning about time and reliability.pdf:application/pdf}
}

@inproceedings{kwiatkowska_stochastic_2007,
	title = {Stochastic model checking},
	booktitle = {International {School} on {Formal} {Methods} for the {Design} of {Computer}, {Communication} and {Software} {Systems}},
	publisher = {Springer},
	author = {Kwiatkowska, Marta and Norman, Gethin and Parker, David},
	year = {2007},
	pages = {220--270},
	file = {Full Text:C\:\\Users\\Lewis\\Zotero\\storage\\JYZ3ZMCI\\Kwiatkowska et al. - 2007 - Stochastic model checking.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\8NECKCSJ\\978-3-540-72522-0_6.html:text/html}
}

@inproceedings{kwiatkowska_prism_2011,
	title = {{PRISM} 4.0: {Verification} of probabilistic real-time systems},
	shorttitle = {{PRISM} 4.0},
	booktitle = {International conference on computer aided verification},
	publisher = {Springer},
	author = {Kwiatkowska, Marta and Norman, Gethin and Parker, David},
	year = {2011},
	pages = {585--591},
	file = {Full Text:C\:\\Users\\Lewis\\Zotero\\storage\\WWECBGSG\\Kwiatkowska et al. - 2011 - PRISM 4.0 Verification of probabilistic real-time.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\5V4A85AU\\978-3-642-22110-1_47.html:text/html}
}

@misc{noauthor_prism_nodate,
	title = {{PRISM} {Manual} {\textbar} {Main} / {Welcome}},
	url = {http://www.prismmodelchecker.org/manual/},
	urldate = {2021-02-25},
	file = {PRISM Manual | Main / Welcome:C\:\\Users\\Lewis\\Zotero\\storage\\WEV69EWC\\manual.html:text/html}
}

@inproceedings{forejt_automated_2011,
	title = {Automated verification techniques for probabilistic systems},
	booktitle = {International school on formal methods for the design of computer, communication and software systems},
	publisher = {Springer},
	author = {Forejt, Vojtěch and Kwiatkowska, Marta and Norman, Gethin and Parker, David},
	year = {2011},
	pages = {53--113},
	file = {Full Text:C\:\\Users\\Lewis\\Zotero\\storage\\BMTURA8B\\Forejt et al. - 2011 - Automated verification techniques for probabilisti.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\R987GJX8\\978-3-642-21455-4_3.html:text/html}
}

@article{kavanagh_gameplay_2020,
	title = {Gameplay {Analysis} of {Multiplayer} {Games} with {Verified} {Action}-{Costs}},
	journal = {The Computer Games Journal},
	author = {Kavanagh, William and Miller, Alice},
	year = {2020},
	note = {Publisher: Springer},
	pages = {1--22},
	file = {Full Text:C\:\\Users\\Lewis\\Zotero\\storage\\76QPFVRT\\s40869-020-00121-5.html:text/html}
}

@misc{wikipedia_deutsch_2006,
	title = {Deutsch:  {Shut} the {Box}},
	copyright = {Public domain},
	shorttitle = {Deutsch},
	url = {https://commons.wikimedia.org/wiki/File:Shut_the_box.jpg},
	urldate = {2021-03-16},
	author = {Wikipedia, Roland Scheicher / Roland Scheicher at German},
	month = jul,
	year = {2006},
	file = {Wikimedia Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\CK7PSLYL\\FileShut_the_box.html:text/html}
}

@incollection{chatterjee_value_2008,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Value {Iteration}},
	isbn = {978-3-540-69850-0},
	url = {https://doi.org/10.1007/978-3-540-69850-0_7},
	abstract = {We survey value iteration algorithms on graphs. Such algorithms can be used for determining the existence of certain paths (model checking), the existence of certain strategies (game solving), and the probabilities of certain events (performance analysis). We classify the algorithms according to the value domain (boolean, probabilistic, or quantitative); according to the graph structure (nondeterministic, probabilistic, or multi-player); according to the desired property of paths (Borel level 1, 2, or 3); and according to the alternation depth and convergence rate of fixpoint computations.},
	language = {en},
	urldate = {2021-03-16},
	booktitle = {25 {Years} of {Model} {Checking}: {History}, {Achievements}, {Perspectives}},
	publisher = {Springer},
	author = {Chatterjee, Krishnendu and Henzinger, Thomas A.},
	editor = {Grumberg, Orna and Veith, Helmut},
	year = {2008},
	doi = {10.1007/978-3-540-69850-0_7},
	keywords = {Improvement Function, Linear Temporal Logic, Model Check, Reward Function, Stochastic Game},
	pages = {107--138},
	file = {Springer Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\AJE6ULG9\\Chatterjee and Henzinger - 2008 - Value Iteration.pdf:application/pdf}
}

@article{haddad_interval_2018,
	title = {Interval {Iteration} {Algorithm} for {MDPs} and {IMDPs}},
	volume = {735},
	url = {https://hal.archives-ouvertes.fr/hal-01809094},
	doi = {10.1016/j.tcs.2016.12.003},
	abstract = {Markov Decision Processes (MDP) are a widely used model including both non-deterministic and probabilistic choices. Minimal and maximal probabilities to reach a target set of states, with respect to a policy resolving non-determinism, may be computed by several methods including value iteration. This algorithm, easy to implement and efficient in terms of space complexity, iteratively computes the probabilities of paths of increasing length. However, it raises three issues: (1) defining a stopping criterion ensuring a bound on the approximation, (2) analysing the rate of convergence, and (3) specifying an additional procedure to obtain the exact values once a sufficient number of iterations has been performed. The first two issues are still open and, for the third one, an upper bound on the number of iterations has been proposed. Based on a graph analysis and transformation of MDPs, we address these problems. First we introduce an interval iteration algorithm, for which the stopping criterion is straightforward. Then we exhibit its convergence rate. Finally we significantly improve the upper bound on the number of iterations required to get the exact values. We extend our approach to also deal with Interval Markov Decision Processes (IMDP) that can be seen as symbolic representations of MDPs.},
	urldate = {2021-03-17},
	journal = {Theoretical Computer Science},
	author = {Haddad, Serge and Monmege, Benjamin},
	month = jul,
	year = {2018},
	note = {Publisher: Elsevier},
	keywords = {Markov decision processes, stochastic verification, value iteration},
	pages = {111 -- 131},
	file = {HAL PDF Full Text:C\:\\Users\\Lewis\\Zotero\\storage\\I62JAAWE\\Haddad and Monmege - 2018 - Interval Iteration Algorithm for MDPs and IMDPs.pdf:application/pdf}
}

@article{norman_verification_2017,
	title = {Verification and control of partially observable probabilistic systems},
	volume = {53},
	issn = {1573-1383},
	url = {https://doi.org/10.1007/s11241-017-9269-4},
	doi = {10.1007/s11241-017-9269-4},
	abstract = {We present automated techniques for the verification and control of partially observable, probabilistic systems for both discrete and dense models of time. For the discrete-time case, we formally model these systems using partially observable Markov decision processes; for dense time, we propose an extension of probabilistic timed automata in which local states are partially visible to an observer or controller. We give probabilistic temporal logics that can express a range of quantitative properties of these models, relating to the probability of an event’s occurrence or the expected value of a reward measure. We then propose techniques to either verify that such a property holds or synthesise a controller for the model which makes it true. Our approach is based on a grid-based abstraction of the uncountable belief space induced by partial observability and, for dense-time models, an integer discretisation of real-time behaviour. The former is necessarily approximate since the underlying problem is undecidable, however we show how both lower and upper bounds on numerical results can be generated. We illustrate the effectiveness of the approach by implementing it in the PRISM model checker and applying it to several case studies from the domains of task and network scheduling, computer security and planning.},
	language = {en},
	number = {3},
	urldate = {2021-03-26},
	journal = {Real-Time Systems},
	author = {Norman, Gethin and Parker, David and Zou, Xueyi},
	month = may,
	year = {2017},
	pages = {354--402},
	file = {Springer Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\ICBFYVMM\\Norman et al. - 2017 - Verification and control of partially observable p.pdf:application/pdf}
}

@article{madani_undecidability_2003,
	series = {Planning with {Uncertainty} and {Incomplete} {Information}},
	title = {On the undecidability of probabilistic planning and related stochastic optimization problems},
	volume = {147},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370202003788},
	doi = {10.1016/S0004-3702(02)00378-8},
	abstract = {Automated planning, the problem of how an agent achieves a goal given a repertoire of actions, is one of the foundational and most widely studied problems in the AI literature. The original formulation of the problem makes strong assumptions regarding the agent's knowledge and control over the world, namely that its information is complete and correct, and that the results of its actions are deterministic and known. Recent research in planning under uncertainty has endeavored to relax these assumptions, providing formal and computation models wherein the agent has incomplete or noisy information about the world and has noisy sensors and effectors. This research has mainly taken one of two approaches: extend the classical planning paradigm to a semantics that admits uncertainty, or adopt another framework for approaching the problem, most commonly the Markov Decision Process (MDP) model. This paper presents a complexity analysis of planning under uncertainty. It begins with the “probabilistic classical planning” problem, showing that problem to be formally undecidable. This fundamental result is then applied to a broad class of stochastic optimization problems, in brief any problem statement where the agent (a) operates over an infinite or indefinite time horizon, and (b) has available only probabilistic information about the system's state. Undecidability is established for policy-existence problems for partially observable infinite-horizon Markov decision processes under discounted and undiscounted total reward models, average-reward models, and state-avoidance models. The results also apply to corresponding approximation problems with undiscounted objective functions. The paper answers a significant open question raised by Papadimitriou and Tsitsiklis [Math. Oper. Res. 12 (3) (1987) 441–450] about the complexity of infinite horizon POMDPs.},
	language = {en},
	number = {1},
	urldate = {2021-03-27},
	journal = {Artificial Intelligence},
	author = {Madani, Omid and Hanks, Steve and Condon, Anne},
	month = jul,
	year = {2003},
	keywords = {Markov decision processes, Computability, Computational complexity, Discounted, Infinity-horizon, Partial observability, Probabilistic planning, Stochastic optimization, Undecidability, Unobservability},
	pages = {5--34},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\GHNHVBWN\\Madani et al. - 2003 - On the undecidability of probabilistic planning an.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\ELG96GXW\\S0004370202003788.html:text/html}
}

@article{bertsekas_approximate_2006,
	title = {Approximate solution methods for partially observable markov and semi-markov decision processes},
	url = {/paper/Approximate-solution-methods-for-partially-markov-Bertsekas-Yu/12ecec6e8443da78e922390b552b343406dc3079},
	abstract = {We consider approximation methods for discrete-time infinite-horizon partially observable Markov and semi-Markov decision processes (POMDP and POSMDP). One of the main contributions of this thesis is a lower cost approximation method for finite-space POMDPs with the average cost criterion, and its extensions to semi-Markov partially observable problems and constrained POMDP problems, as well as to problems with the undiscounted total cost criterion. 
Our method is an extension of several lower cost approximation schemes, proposed individually by various authors, for discounted POMDP problems. We introduce a unified framework for viewing all of these schemes together with some new ones. In particular, we establish that due to the special structure of hidden states in a POMDP, there is a class of approximating processes, which are either POMDPs or belief MDPs, that provide lower bounds to the optimal cost function of the original POMDP problem. 
Theoretically, POMDPs with the long-run average cost criterion are still not fully understood. The major difficulties relate to the structure of the optimal solutions, such as conditions for a constant optimal cost function, the existence of solutions to the optimality equations, and the existence of optimal policies that are stationary and deterministic. Thus, our lower bound result is useful not only in providing a computational method, but also in characterizing the optimal solution. We show that regardless of these theoretical difficulties, lower bounds of the optimal liminf average cost function can be computed efficiently by solving modified problems using multichain MDP algorithms, and the approximating cost functions can be also used to obtain suboptimal stationary control policies. We prove the asymptotic convergence of the lower bounds under certain assumptions. 
For semi-Markov problems and total cost problems, we show that the same method can be applied for computing lower bounds of the optimal cost function. For constrained average cost POMDPs, we show that lower bounds of the constrained optimal cost function can be computed by solving finite-dimensional LPs. 
We also consider reinforcement learning methods for POMDPs and MDPs. We propose an actor-critic type policy gradient algorithm that uses a structured policy known as a finite-state controller. We thus provide an alternative to the earlier actor-only algorithm GPOMDP. Our work also clarifies the relationship between the reinforcement learning methods for POMDPs and those for MDPs. For average cost MDPs, we provide a convergence and convergence rate analysis for a least squares temporal difference (TD) algorithm, called LSPE, and previously proposed for discounted problems. We use this algorithm in the critic portion of the policy gradient algorithm for POMDPs with finite-state controllers. 
Finally, we investigate the properties of the limsup and liminf average cost functions of various types of policies. We show various convexity and concavity properties of these cost functions, and we give a new necessary condition for the optimal liminf average cost to be constant. Based on this condition, we prove the near-optimality of the class of finite-state controllers under the assumption of a constant optimal liminf average cost. This result provides a theoretical guarantee for the finite-state controller approach. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)},
	language = {en},
	urldate = {2021-03-28},
	journal = {undefined},
	author = {Bertsekas, D. and Yu, Huizhen},
	year = {2006},
	file = {Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\NHEQVBV8\\12ecec6e8443da78e922390b552b343406dc3079.html:text/html}
}

@article{lovejoy_computationally_1991,
	title = {Computationally {Feasible} {Bounds} for {Partially} {Observed} {Markov} {Decision} {Processes}},
	volume = {39},
	issn = {0030-364X},
	url = {https://www.jstor.org/stable/171496},
	abstract = {A partially observed Markov decision process (POMDP) is a sequential decision problem where information concerning parameters of interest is incomplete, and possible actions include sampling, surveying, or otherwise collecting additional information. Such problems can theoretically be solved as dynamic programs, but the relevant state space is infinite, which inhibits algorithmic solution. This paper explains how to approximate the state space by a finite grid of points, and use that grid to construct upper and lower value function bounds, generate approximate nonstationary and stationary policies, and bound the value loss relative to optimal for using these policies in the decision problem. A numerical example illustrates the methodology.},
	number = {1},
	urldate = {2021-03-28},
	journal = {Operations Research},
	author = {Lovejoy, William S.},
	year = {1991},
	note = {Publisher: INFORMS},
	pages = {162--175},
	file = {JSTOR Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\3UB64MR5\\Lovejoy - 1991 - Computationally Feasible Bounds for Partially Obse.pdf:application/pdf}
}

@article{hansen_dynamic_nodate,
	title = {Dynamic {Programming} for {Partially} {Observable} {Stochastic} {Games}},
	abstract = {We develop an exact dynamic programming algorithm for partially observable stochastic games (POSGs). The algorithm is a synthesis of dynamic programming for partially observable Markov decision processes (POMDPs) and iterated elimination of dominated strategies in normal form games. We prove that when applied to ﬁnite-horizon POSGs, the algorithm iteratively eliminates very weakly dominated strategies without ﬁrst forming a normal form representation of the game. For the special case in which agents share the same payoffs, the algorithm can be used to ﬁnd an optimal solution. We present preliminary empirical results and discuss ways to further exploit POMDP theory in solving POSGs.},
	language = {en},
	author = {Hansen, Eric A and Bernstein, Daniel S and Zilberstein, Shlomo},
	pages = {7},
	file = {Hansen et al. - Dynamic Programming for Partially Observable Stoch.pdf:C\:\\Users\\Lewis\\Zotero\\storage\\JZXVS6AW\\Hansen et al. - Dynamic Programming for Partially Observable Stoch.pdf:application/pdf}
}

@inproceedings{kwiatkowska_automated_2018,
	title = {Automated verification of concurrent stochastic games},
	booktitle = {International {Conference} on {Quantitative} {Evaluation} of {Systems}},
	publisher = {Springer},
	author = {Kwiatkowska, Marta and Norman, Gethin and Parker, David and Santos, Gabriel},
	year = {2018},
	pages = {223--239},
	file = {Full Text:C\:\\Users\\Lewis\\Zotero\\storage\\4FXF7E9Z\\Kwiatkowska et al. - 2018 - Automated verification of concurrent stochastic ga.pdf:application/pdf;Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\MFHL8N82\\978-3-319-99154-2_14.html:text/html}
}

@article{v_neumann_zur_1928,
	title = {Zur {Theorie} der {Gesellschaftsspiele}},
	volume = {100},
	issn = {1432-1807},
	url = {https://doi.org/10.1007/BF01448847},
	doi = {10.1007/BF01448847},
	language = {de},
	number = {1},
	urldate = {2021-04-05},
	journal = {Mathematische Annalen},
	author = {v. Neumann, J.},
	month = dec,
	year = {1928},
	pages = {295--320},
	file = {Springer Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\9F3GQPCU\\v. Neumann - 1928 - Zur Theorie der Gesellschaftsspiele.pdf:application/pdf}
}

@misc{noauthor_snowball_nodate,
	title = {The {Snowball} {Effect} ({And} {How} to {Avoid} {It}) in {Game} {Design}},
	url = {https://gamedevelopment.tutsplus.com/articles/the-snowball-effect-and-how-to-avoid-it-in-game-design--cms-21892},
	abstract = {We play games to have fun. Most (but not all) games use some sort of competition to drive this fun: from chess to Street Fighter, players enjoy pitting their skills against each other. A...},
	urldate = {2021-04-06},
	journal = {Game Development Envato Tuts+},
	file = {Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\DT2GUQAQ\\the-snowball-effect-and-how-to-avoid-it-in-game-design--cms-21892.html:text/html}
}

@article{michie_experiments_1963,
	title = {Experiments on the {Mechanization} of {Game}-{Learning} {Part} {I}. {Characterization} of the {Model} and its parameters},
	volume = {6},
	issn = {0010-4620},
	url = {https://doi.org/10.1093/comjnl/6.3.232},
	doi = {10.1093/comjnl/6.3.232},
	abstract = {This paper describes a trial-and-error device which learns to play the game of Noughts and Crosses. It was initially constructed from matchboxes and coloured beads and subsequently simulated in essentials by a program for a Pegasus 2 computer. The parameters governing the adaptive behaviour of this automaton are described and preliminary observations on its performance are briefly reported.},
	number = {3},
	urldate = {2021-04-07},
	journal = {The Computer Journal},
	author = {Michie, Donald},
	month = nov,
	year = {1963},
	pages = {232--236},
	file = {Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\EQRFXQFH\\360077.html:text/html;Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\UC2YVKYJ\\Michie - 1963 - Experiments on the Mechanization of Game-Learning .pdf:application/pdf}
}

@incollection{kwiatkowska_verification_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Verification and {Control} of {Turn}-{Based} {Probabilistic} {Real}-{Time} {Games}},
	isbn = {978-3-030-31175-9},
	url = {https://doi.org/10.1007/978-3-030-31175-9_22},
	abstract = {Quantitative verification techniques have been developed for the formal analysis of a variety of probabilistic models, such as Markov chains, Markov decision process and their variants. They can be used to produce guarantees on quantitative aspects of system behaviour, for example safety, reliability and performance, or to help synthesise controllers that ensure such guarantees are met. We propose the model of turn-based probabilistic timed multi-player games, which incorporates probabilistic choice, real-time clocks and nondeterministic behaviour across multiple players. Building on the digital clocks approach for the simpler model of probabilistic timed automata, we show how to compute the key measures that underlie quantitative verification, namely the probability and expected cumulative price to reach a target. We illustrate this on case studies from computer security and task scheduling.},
	language = {en},
	urldate = {2021-04-07},
	booktitle = {The {Art} of {Modelling} {Computational} {Systems}: {A} {Journey} from {Logic} and {Concurrency} to {Security} and {Privacy}: {Essays} {Dedicated} to {Catuscia} {Palamidessi} on the {Occasion} of {Her} 60th {Birthday}},
	publisher = {Springer International Publishing},
	author = {Kwiatkowska, Marta and Norman, Gethin and Parker, David},
	editor = {Alvim, Mário S. and Chatzikokolakis, Kostas and Olarte, Carlos and Valencia, Frank},
	year = {2019},
	doi = {10.1007/978-3-030-31175-9_22},
	pages = {379--396},
	file = {Springer Full Text PDF:C\:\\Users\\Lewis\\Zotero\\storage\\46VWWJTT\\Kwiatkowska et al. - 2019 - Verification and Control of Turn-Based Probabilist.pdf:application/pdf}
}

@book{demmel_applied_1997,
	series = {Other {Titles} in {Applied} {Mathematics}},
	title = {Applied {Numerical} {Linear} {Algebra}},
	isbn = {978-0-89871-389-3},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611971446},
	abstract = {This textbook covers both direct and iterative methods for the solution of linear systems, least squares problems, eigenproblems, and the singular value decomposition. Earlier versions have been used by the author in graduate classes in the Mathematics Department of the University of California at Berkeley since 1990 and at the Courant Institute before then. In writing this textbook I aspired to meet the following goals: 1. The text should be attractive to first-year graduate students from a variety of engineering and scientific disciplines. 2. It should be self-contained, assuming only a good undergraduate background in linear algebra. 3. The students should learn the mathematical basis of the field, as well as how to build or find good numerical software. 4. Students should acquire practical knowledge for solving real problems efficiently. In particular, they should know what the state-of-the-art techniques are in each area or when to look for them and where to find them, even if I analyze only simpler versions in the text. 5. It should all fit in one semester, since that is what most students have available for this subject. Indeed, I was motivated to write this book because the available textbooks, while very good, did not meet these goals. Golub and Van Loan's text [121] is too encyclopedic in style, while still omitting some important topics such as multigrid, domain decomposition, and some recent algorithms for eigenvalue problems. Watkins's [252] and Trefethen's and Bau's [243] also omit some state-of-the-art algorithms.},
	urldate = {2021-04-08},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Demmel, James W.},
	month = jan,
	year = {1997},
	doi = {10.1137/1.9781611971446},
	file = {Snapshot:C\:\\Users\\Lewis\\Zotero\\storage\\S48MRP9P\\1.html:text/html}
}
